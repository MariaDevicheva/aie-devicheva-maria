from __future__ import annotations

from pathlib import Path
from typing import Optional

import pandas as pd
import typer

from .core import (
    DatasetSummary,
    compute_quality_flags,
    correlation_matrix,
    flatten_summary_for_print,
    missing_table,
    summarize_dataset,
    top_categories,
    get_quality_report,
    generate_json_summary,
    save_json_summary,
)
from .viz import (
    plot_correlation_heatmap,
    plot_missing_matrix,
    plot_histograms_per_column,
    save_top_categories_tables,
)

app = typer.Typer(help="–ú–∏–Ω–∏-CLI –¥–ª—è EDA CSV-—Ñ–∞–π–ª–æ–≤")


def _load_csv(
    path: Path,
    sep: str = ",",
    encoding: str = "utf-8",
) -> pd.DataFrame:
    if not path.exists():
        raise typer.BadParameter(f"–§–∞–π–ª '{path}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
    try:
        return pd.read_csv(path, sep=sep, encoding=encoding)
    except Exception as exc:
        raise typer.BadParameter(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV: {exc}") from exc


@app.command()
def overview(
    path: str = typer.Argument(..., help="–ü—É—Ç—å –∫ CSV-—Ñ–∞–π–ª—É."),
    sep: str = typer.Option(",", help="–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV."),
    encoding: str = typer.Option("utf-8", help="–ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–∞."),
) -> None:
    df = _load_csv(Path(path), sep=sep, encoding=encoding)
    summary: DatasetSummary = summarize_dataset(df)
    summary_df = flatten_summary_for_print(summary)

    typer.echo(f"–°—Ç—Ä–æ–∫: {summary.n_rows}")
    typer.echo(f"–°—Ç–æ–ª–±—Ü–æ–≤: {summary.n_cols}")
    typer.echo("\n–ö–æ–ª–æ–Ω–∫–∏:")
    typer.echo(summary_df.to_string(index=False))


@app.command()
def report(
    path: str = typer.Argument(..., help="–ü—É—Ç—å –∫ CSV-—Ñ–∞–π–ª—É."),
    out_dir: str = typer.Option("reports", help="–ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è –æ—Ç—á—ë—Ç–∞."),
    sep: str = typer.Option(",", help="–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ CSV."),
    encoding: str = typer.Option("utf-8", help="–ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–∞."),
    max_hist_columns: int = typer.Option(6, help="–ú–∞–∫—Å–∏–º—É–º —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º."),
    top_k_categories: int = typer.Option(10, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ top-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."),
    title: str = typer.Option("–û—Ç—á–µ—Ç EDA", help="–ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á—ë—Ç–∞."),
    min_missing_share: float = typer.Option(
        0.3,
        min=0.0,
        max=1.0,
        help="–ü–æ—Ä–æ–≥ –¥–æ–ª–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫."
    ),
    quality_threshold: float = typer.Option(
        0.6,
        min=0.0,
        max=1.0,
        help="–ü–æ—Ä–æ–≥ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."
    ),
    json_summary: bool = typer.Option(False, help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–º–ø–∞–∫—Ç–Ω—É—é JSON-—Å–≤–æ–¥–∫—É –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É."),
) -> None:
    out_root = Path(out_dir)
    out_root.mkdir(parents=True, exist_ok=True)

    df = _load_csv(Path(path), sep=sep, encoding=encoding)

    summary = summarize_dataset(df)
    summary_df = flatten_summary_for_print(summary)
    missing_df = missing_table(df)
    corr_df = correlation_matrix(df)
    top_cats = top_categories(df, max_columns=5, top_k=top_k_categories)

    # üî• –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ø–µ—Ä–µ–¥–∞—ë–º df –∏ min_missing_share
    quality_flags = compute_quality_flags(
        summary,
        missing_df,
        df=df,  # ‚Üê —Ç–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞—ë–º –¥–∞–Ω–Ω—ã–µ
        min_missing_share=min_missing_share  # ‚Üê –∏ –ø–æ—Ä–æ–≥
    )

    summary_df.to_csv(out_root / "summary.csv", index=False)
    if not missing_df.empty:
        missing_df.to_csv(out_root / "missing.csv", index=True)
    if not corr_df.empty:
        corr_df.to_csv(out_root / "correlation.csv", index=True)
    save_top_categories_tables(top_cats, out_root / "top_categories")

    if json_summary:
        cli_params = {
            "max_hist_columns": max_hist_columns,
            "top_k_categories": top_k_categories,
            "title": title,
            "min_missing_share": min_missing_share,
            "quality_threshold": quality_threshold,
            "json_summary": json_summary,
            "sep": sep,
            "encoding": encoding
        }
        json_data = generate_json_summary(
            summary=summary,
            missing_df=missing_df,
            quality_flags=quality_flags,
            top_categories=top_cats,
            corr_matrix=corr_df,
            params=cli_params
        )
        json_path = save_json_summary(json_data, out_root / "summary.json")
        typer.echo(f"JSON-—Å–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {json_path}")

    md_path = out_root / "report.md"
    with md_path.open("w", encoding="utf-8") as f:
        f.write(f"# {title}\n\n")
        f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: `{Path(path).name}`\n")
        f.write(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç—á–µ—Ç–∞\n")
        f.write(f"- –ú–∞–∫—Å–∏–º—É–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º: `{max_hist_columns}`\n")
        f.write(f"- Top –∫–∞—Ç–µ–≥–æ—Ä–∏–π: `{top_k_categories}`\n")
        f.write(f"- –ü–æ—Ä–æ–≥ –ø—Ä–æ–ø—É—Å–∫–æ–≤: `{min_missing_share:.0%}`\n")
        f.write(f"- –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞: `{quality_threshold:.0%}`\n")
        f.write(f"- JSON-—Å–≤–æ–¥–∫–∞: `{'–î–∞' if json_summary else '–ù–µ—Ç'}`\n\n")

        f.write(f"–°—Ç—Ä–æ–∫: **{summary.n_rows}**, —Å—Ç–æ–ª–±—Ü–æ–≤: **{summary.n_cols}**\n\n")

        f.write("## –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö\n\n")
        quality_report = get_quality_report(quality_flags)
        f.write(quality_report)
        f.write("\n\n")

        f.write(f"### –ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ >{min_missing_share:.0%}\n\n")
        problematic_cols = missing_df[missing_df["missing_share"] > min_missing_share]
        if not problematic_cols.empty:
            f.write("| –ö–æ–ª–æ–Ω–∫–∞ | –ü—Ä–æ–ø—É—Å–∫–æ–≤ | –î–æ–ª—è |\n")
            f.write("|---------|-----------|------|\n")
            for idx, row in problematic_cols.iterrows():
                f.write(f"| {idx} | {int(row['missing_count'])} | {row['missing_share']:.1%} |\n")
        else:
            f.write(f"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –±–æ–ª–µ–µ {min_missing_share:.0%}.\n")
        f.write("\n")

        f.write("### –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º\n\n")

        if quality_flags.get("has_constant_columns", False):
            f.write("**–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:**\n")
            for col in quality_flags.get("constant_columns", []):
                f.write(f"- `{col}` - —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ\n")
            f.write("\n")

        if quality_flags.get("has_high_cardinality_categoricals", False):
            f.write("**–ö–æ–ª–æ–Ω–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é:**\n")
            for col_info in quality_flags.get("high_cardinality_columns", []):
                f.write(f"- `{col_info['name']}`: {col_info['unique']} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ")
                f.write(f"({col_info['unique_ratio']:.1%} –æ—Ç –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫)\n")
            f.write("\n")

        if quality_flags.get("suspicious_id_duplicates", False):
            f.write("**–í–Ω–∏–º–∞–Ω–∏–µ:** –í–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ ID-–∫–æ–ª–æ–Ω–∫–µ\n\n")

        if quality_flags.get("has_many_zero_values", False):
            f.write("**–ö–æ–ª–æ–Ω–∫–∏ —Å –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:**\n")
            for col in quality_flags.get("many_zero_columns", []):
                f.write(f"- `{col}`\n")
            f.write("\n")

        f.write("## –ö–æ–ª–æ–Ω–∫–∏\n\n")
        f.write("–°–º. —Ñ–∞–π–ª `summary.csv`.\n\n")

        f.write("## –ü—Ä–æ–ø—É—Å–∫–∏\n\n")
        if missing_df.empty:
            f.write("–ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç –∏–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç.\n\n")
        else:
            f.write(f"–í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: {len(missing_df[missing_df['missing_count'] > 0])}\n")
            f.write("–°–º. —Ñ–∞–π–ª—ã `missing.csv` –∏ `missing_matrix.png`.\n\n")

        f.write("## –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n\n")
        if not top_cats:
            f.write("–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ/—Å—Ç—Ä–æ–∫–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.\n\n")
        else:
            f.write(f"*–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è top-{top_k_categories} –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏*\n\n")
            for col_name, cat_df in top_cats.items():
                f.write(f"### {col_name}\n")
                f.write("| –ó–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –î–æ–ª—è |\n")
                f.write("|----------|------------|------|\n")
                for _, row in cat_df.iterrows():
                    f.write(f"| {row['value']} | {int(row['count'])} | {row['share']:.1%} |\n")
                f.write(f"\n*–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {df[col_name].nunique()}*\n\n")
            f.write("–ü–æ–ª–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã —Å–º. –≤ –ø–∞–ø–∫–µ `top_categories/`.\n\n")

        f.write("## –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n\n")
        numeric_cols = [col.name for col in summary.columns if col.is_numeric]
        if numeric_cols:
            f.write(f"**–í—Å–µ–≥–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫:** {len(numeric_cols)}\n")
            f.write(f"**–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è:** {min(max_hist_columns, len(numeric_cols))} –∫–æ–ª–æ–Ω–æ–∫\n\n")
            f.write("### –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n")
            f.write("| –ö–æ–ª–æ–Ω–∫–∞ | Min | Max | Mean | Std | –ü—Ä–æ–ø—É—Å–∫–∏ |\n")
            f.write("|---------|-----|-----|------|-----|----------|\n")
            for col in summary.columns:
                if col.is_numeric:
                    missing_pct = f"{col.missing_share:.1%}" if col.missing > 0 else "0%"
                    f.write(f"| {col.name} | {col.min or '‚Äî'} | {col.max or '‚Äî'} | ")
                    f.write(f"{col.mean or '‚Äî'} | {col.std or '‚Äî'} | {missing_pct} |\n")
            f.write("\n")
            f.write("### –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã\n")
            f.write("–°–º. —Ñ–∞–π–ª—ã `hist_*.png`.\n\n")

        f.write("## –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n\n")
        if corr_df.empty:
            f.write("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏.\n\n")
        else:
            f.write("–°–º. `correlation.csv` –∏ `correlation_heatmap.png`.\n\n")
            strong_corrs = []
            for i in range(len(corr_df.columns)):
                for j in range(i + 1, len(corr_df.columns)):
                    corr = corr_df.iloc[i, j]
                    if abs(corr) > 0.7:
                        col1 = corr_df.columns[i]
                        col2 = corr_df.columns[j]
                        strong_corrs.append((col1, col2, corr))
            if strong_corrs:
                f.write("### –°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (|r| > 0.7)\n")
                f.write("| –ö–æ–ª–æ–Ω–∫–∞ 1 | –ö–æ–ª–æ–Ω–∫–∞ 2 | –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç |\n")
                f.write("|-----------|-----------|-------------|\n")
                for col1, col2, corr in strong_corrs[:5]:
                    f.write(f"| {col1} | {col2} | {corr:.3f} |\n")
                f.write("\n")

        f.write("## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n\n")
        f.write(f"**–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞:** {quality_flags['quality_score']:.3f} ")
        f.write(f"({quality_flags['quality_category']})\n\n")

        if quality_flags['quality_score'] >= quality_threshold:
            f.write("**–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ** –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n")
        else:
            f.write("**–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è** –ø–µ—Ä–µ–¥ –∞–Ω–∞–ª–∏–∑–æ–º.\n")

        f.write("\n### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n")
        recommendations = []

        if quality_flags.get("has_constant_columns", False):
            recommendations.append("–£–¥–∞–ª–∏—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏")
        if quality_flags.get("has_very_high_missing_cols", False):
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏ —Å >90% –ø—Ä–æ–ø—É—Å–∫–æ–≤")
        if problematic_cols.shape[0] > 0:
            recommendations.append(f"–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ {problematic_cols.shape[0]} –∫–æ–ª–æ–Ω–∫–∞—Ö")
        if quality_flags.get("suspicious_id_duplicates", False):
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å ID")
        if quality_flags.get("has_high_cardinality_categoricals", False):
            recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é")

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                f.write(f"{i}. {rec}\n")
        else:
            f.write("–î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.\n")

    plot_histograms_per_column(df, out_root, max_columns=max_hist_columns)
    plot_missing_matrix(df, out_root / "missing_matrix.png")
    plot_correlation_heatmap(df, out_root / "correlation_heatmap.png")

    typer.echo(f"–û—Ç—á—ë—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –≤ –∫–∞—Ç–∞–ª–æ–≥–µ: {out_root}")
    typer.echo(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: '{title}'")
    typer.echo(f"   –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
    typer.echo(f"     - –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã: {max_hist_columns} –∫–æ–ª–æ–Ω–æ–∫")
    typer.echo(f"     - Top –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {top_k_categories}")
    typer.echo(f"     - –ü–æ—Ä–æ–≥ –ø—Ä–æ–ø—É—Å–∫–æ–≤: {min_missing_share:.0%}")
    typer.echo(f"     - –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞: {quality_threshold:.0%}")
    typer.echo(f"     - JSON-—Å–≤–æ–¥–∫–∞: {'–î–∞' if json_summary else '–ù–µ—Ç'}")

    if json_summary:
        typer.echo(f"\nJSON-—Å–≤–æ–¥–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç:")
        typer.echo(f"   - –†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞: {summary.n_rows} —Å—Ç—Ä–æ–∫, {summary.n_cols} –∫–æ–ª–æ–Ω–æ–∫")
        typer.echo(f"   - –û—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞: {quality_flags['quality_score']:.3f}")
        typer.echo(f"   - –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫")
        typer.echo(f"   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞")

    typer.echo(f"\n–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    typer.echo(f"   - {md_path}")
    if json_summary:
        typer.echo(f"   - {out_root / 'summary.json'}")
    typer.echo(f"   - –¢–∞–±–ª–∏—Ü—ã: summary.csv, missing.csv, correlation.csv")
    typer.echo(f"   - –ì—Ä–∞—Ñ–∏–∫–∏: hist_*.png, missing_matrix.png, correlation_heatmap.png")


if __name__ == "__main__":
    app()